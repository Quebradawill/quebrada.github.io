---
layout: post
title: 详解熵、条件熵、相对熵和交叉熵
date: 2019-12-29
categories: Mathematics
tags: Information
status: publish
type: post
published: true
author: Quebradawill
---

[toc]

### 1. 信息熵（Information Entropy）

一条信息的信息量大小和它的**不确定性**有直接的关系。我们需要搞清楚一件非常非常不确定的事，或者是我们一无所知的事，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们就不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，**信息量的度量就等于不确定性的多少**。

考虑一个**离散的随机变量** $X$，信息的量度应该依赖于概率分布 $P(X)$，因此我们想要寻找一个函数 $I(X)$，它是概率 $P(X)$ 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 $X$ 和 $Y$，那么观察这两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：$I(X,Y)=I(X)+I(Y)$。因为这两个事件是独立不相关的，因此 $P(X,Y)=P(X)P(Y)$。根据这两个关系，很容易看出 $I(X)$一定与 $P(X)$ 的**对数**有关（因为对数的运算法则是 $\log (mn) = \log m + \log n$）。因此，我们有 $I(X)=− \log P(X)$。其中<font color='blue'>负号</font>是用来保证**信息量是正数或者零**。而 $\log$ 函数基的选择是任意的（信息论中基常常选择为 $2$，因此信息的单位为比特 bits；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特 nats）。$I(X)$ 也被称为随机变量 $X$ 的**自信息（self-information）**，描述的是随机变量的某个事件发生所带来的信息量。

最后，我们正式引出<font color='red'>信息熵</font>。 现在假设一个发送者想传送一个随机变量的值给接收者。那么在这个过程中，他们传输的平均信息量可以通过求 $I(x)=− \log p(x)$ 关于概率分布 $p(x)$ 的期望得到，即：


$$
H(X) = - \sum_x p(x) \log p(x) = - \sum_{i=1}^n p(x_i) \log p(x_i)
$$


$H(X)$ 就被称为随机变量 $X$ 的**熵**，它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。从公式可得，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大，且 $ 0 \leq H(X) \leq \log n$。稍后证明。

将一维随机变量分布推广到多维随机变量分布，则其<font color='red'>联合熵（Joint entropy）</font>为：


$$
H(X,Y) = -\sum_{x,y} p(x,y) \log p(x,y) = - \sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j) \log p(x_i, y_j)
$$


**注意：**熵只依赖于随机变量的分布，与随机变量取值无关，所以也可以将 $X$ 的熵记作 $H(p)$。令 $0 \log 0 = 0$（因为某个取值的概率可能为 $0$）。

Shannon 编码定理表明**熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）**。

对 $ 0 \leq H(X) \leq \log n$ 的证明。

**证明：**利用拉格朗日乘子法。约束条件：

 
$$
p(x_1) + p(x_2) + \cdots + p(x_n) = 1
$$


目标函数为：


$$
f(p(x_1), p(x_2), \cdots, p(x_n)) = - \left( p(x_1) \log p(x_1) + p(x_2) \log p(x_2) +  \cdots + p(x_n) \log p(x_n) \right)
$$


定义拉格朗日函数：


$$
\begin{align} {\notag} L(p(x_1), p(x_2), \cdots, p(x_n), \lambda) = & - \left( p(x_1) \log p(x_1) + p(x_2) \log p(x_2) +  \cdots + p(x_n) \log p(x_n) \right) \\ & + \lambda \left(p(x_1) + p(x_2) + \cdots + p(x_n) - 1 \right) \end{align}
$$


分别对 $p(x_1), p(x_2), \cdots, p(x_n), \lambda$ 求偏导数，令偏导数为 $0$，可得（此处假设为自然对数 $\ln$）


$$
\begin{align} {\notag} - \log p(x_1) - 1 + \lambda = 0 \Longrightarrow & \lambda - \log \left( e \cdot p(x_1) \right) = 0 \\ & \lambda - \log \left( e \cdot p(x_2) \right) = 0 \\ & \cdots \\ & \lambda - \log \left( e \cdot p(x_n) \right) = 0 \\ & p(x_1) + p(x_2) + \cdots + p(x_n) - 1 = 0 \end{align}
$$


求出 $p(x_1), p(x_2), \cdots, p(x_n)$ 的值，解方程可得 $p(x_1) = p(x_2) = \cdots = p(x_n) = 1/n$，代入目标函数可得极值


$$
f \left( \frac{1}{n}, \frac{1}{n}, \cdots, \frac{1}{n} \right) = - \left( \frac{1}{n} \log \frac{1}{n} +  \frac{1}{n} \log \frac{1}{n} + \cdots + \frac{1}{n} \log \frac{1}{n} \right) = - \log \frac{1}{n} = \log n
$$


### 2. 条件熵（Conditional Entropy）

<font color='red'>条件熵</font> $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性。条件熵 $H(Y|X)$ 定义为 $X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望：


$$
\begin{align} {\notag} H(Y | X) & = \sum_x p(x) H(Y | X = x) \\ & = - \sum_x p(x) \sum_y p(y | x) \log p(y|x) \\ & = - \sum_x \sum_y p(x,y) \log p(y|x) \\ & = - \sum_{x,y} p(x,y) \log p(y|x) \end{align}
$$


条件熵 $H(Y|X)$ 相当于联合熵 $H(X,Y)$ 减去单独的熵 $H(X)$，即 $H(Y|X) = H(X,Y) - H(X)$，证明如下：


$$
\begin{aligned} {\notag} H(X,Y) & = - \sum_{x,y} p(x,y) \log p(x,y) \\ & = - \sum_{x,y} p(x,y) \log \left( p(y|x) p(x) \right) \\ & = - \sum_{x,y} p(x,y) \log p(y|x) - \sum_{x,y} p(x,y) \log p(x) \\ & = H(Y|X) - \sum_{x,y} p(x,y) \log p(x) \\ & = H(Y|X) - \sum_x \sum_y p(x,y) \log p(x) \\ & = H(Y|X) - \sum_x \log p(x) \sum_y p(x,y) \\ & = H(Y|X) - \sum_x \log p(x) p(x) \\ & = H(Y|X) + H(X) \end{aligned}
$$


举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布 $H(X,Y)$，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设 $H(X)$ 对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知 $H(X)$ 这个信息量的时候，$H(X,Y)$ 剩下的信息量就是条件熵：$H(Y|X)=H(X,Y)−H(X)$。

因此，可以这样理解，描述 $X$ 和 $Y$ 所需的信息是描述 $X$ 自己所需的信息，加上给定 $X$ 的条件下具体化 $Y$ 所需的额外信息。

### 3. 相对熵（Relative Entropy）

也称为 **KL 散度**（Kullback-Leibler divergence）。

设 $p(x)$、$q(x)$ 是离散型随机变量 $X$ 中取值的两个概率分布，则 $p$ 对 $q$ 的相对熵是：


$$
D_{\rm{KL}} (p \Vert q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = E_{p(x)} \log \frac{p(x)}{q(x)}
$$
<font color='blue'>最后应该表示的是在概率 $p(x)$ 下 $\log \frac{p(x)}{q(x)}$ 的期望值吧？</font>

**性质：**

- 如果 $p(x)$ 和 $q(x)$ 两个分布相同，那么相对熵等于 $0$；

- $D_{\rm{KL}} (p \Vert q) \neq D_{\rm{KL}} (q \Vert p)$，即相对熵不具有对称性；

- $D_{\rm{KL}} (p \Vert q) \geq 0$，用 Jensen 不等式证明如下：
  $$
  H(X,Y)
  $$
  

#### 参考：

1、[详解机器学习中的熵、条件熵、相对熵和交叉熵](https://zhuanlan.zhihu.com/p/35379531)

