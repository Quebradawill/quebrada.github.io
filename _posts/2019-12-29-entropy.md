---
layout: post
title: 详解熵、条件熵、相对熵和交叉熵
date: 2019-12-29
categories: Mathematics
tags: Information
status: publish
type: post
published: true
author: Quebradawill
---

[toc]

### 1. 信息熵（Information Entropy）

一条信息的信息量大小和它的**不确定性**有直接的关系。我们需要搞清楚一件非常非常不确定的事，或者是我们一无所知的事，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们就不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，**信息量的度量就等于不确定性的多少**。

考虑一个**离散的随机变量** $X$，信息的量度应该依赖于概率分布 $P(X)$，因此我们想要寻找一个函数 $I(X)$，它是概率 $P(X)$ 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 $X$ 和 $Y$，那么观察这两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：$I(X,Y)=I(X)+I(Y)$。因为这两个事件是独立不相关的，因此 $P(X,Y)=P(X)P(Y)$。根据这两个关系，很容易看出 $I(X)$一定与 $P(X)$ 的**对数**有关（因为对数的运算法则是 $\log (mn) = \log m + \log n$）。因此，我们有 $I(X)=− \log P(X)$。其中<font color='blue'>负号</font>是用来保证**信息量是正数或者零**。而 $\log$ 函数基的选择是任意的（信息论中基常常选择为 $2$，因此信息的单位为比特 bits；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特 nats）。$I(X)$ 也被称为随机变量 $X$ 的**自信息（self-information）**，描述的是随机变量的某个事件发生所带来的信息量。

最后，我们正式引出<font color='red'>信息熵</font>。 现在假设一个发送者想传送一个随机变量的值给接收者。那么在这个过程中，他们传输的平均信息量可以通过求 $I(x)=− \log p(x)$ 关于概率分布 $p(x)$ 的期望得到，即：


$$
H(X) = - \sum_x p(x) \log p(x) = - \sum_{i=1}^n p(x_i) \log p(x_i)
$$


$H(X)$ 就被称为随机变量 $X$ 的**熵**，它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。从公式可得，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大，且 $ 0 \leq H(X) \leq \log n$。稍后证明。

将一维随机变量分布推广到多维随机变量分布，则其<font color='red'>联合熵（Joint entropy）</font>为：


$$
H(X,Y) = -\sum_{x,y} p(x,y) \log p(x,y) = - \sum_{i=1}^n \sum_{j=1}^m p(x_i, y_j) \log p(x_i, y_j)
$$


**注意：**熵只依赖于随机变量的分布，与随机变量取值无关，所以也可以将 $X$ 的熵记作 $H(p)$。令 $0 \log 0 = 0$（因为某个取值的概率可能为 $0$）。

Shannon 编码定理表明**熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度）**。

对 $ 0 \leq H(X) \leq \log n$ 的证明。

**证明：**利用拉格朗日乘子法。约束条件：

 
$$
p(x_1) + p(x_2) + \cdots + p(x_n) = 1
$$


目标函数为：


$$
f(p(x_1), p(x_2), \cdots, p(x_n)) = - \left( p(x_1) \log p(x_1) + p(x_2) \log p(x_2) +  \cdots + p(x_n) \log p(x_n) \right)
$$


定义拉格朗日函数：


$$
\begin{align} L(p(x_1), p(x_2), \cdots, p(x_n), \lambda) = & - \left( p(x_1) \log p(x_1) + p(x_2) \log p(x_2) +  \cdots + p(x_n) \log p(x_n) \right) \\ & + \lambda \left(p(x_1) + p(x_2) + \cdots + p(x_n) - 1 \right) \end{align}
$$


分别对 $p(x_1), p(x_2), \cdots, p(x_n), \lambda$ 求偏导数，令偏导数为 $0$，可得（此处假设为自然对数 $\ln$）


$$
\begin{align} - \log p(x_1) - 1 + \lambda = 0 \Longrightarrow & \lambda - \log \left( e \cdot p(x_1) \right) = 0 \\ & \lambda - \log \left( e \cdot p(x_2) \right) = 0 \\ & \cdots \\ & \lambda - \log \left( e \cdot p(x_n) \right) = 0 \\ & p(x_1) + p(x_2) + \cdots + p(x_n) - 1 = 0 \end{align}
$$


求出 $p(x_1), p(x_2), \cdots, p(x_n)$ 的值，解方程可得 $p(x_1) = p(x_2) = \cdots = p(x_n) = 1/n$，代入目标函数可得极值


$$
f \left( \frac{1}{n}, \frac{1}{n}, \cdots, \frac{1}{n} \right) = - \left( \frac{1}{n} \log \frac{1}{n} +  \frac{1}{n} \log \frac{1}{n} + \cdots + \frac{1}{n} \log \frac{1}{n} \right) = - \log \frac{1}{n} = \log n
$$


### 2. 条件熵（Conditional Entropy）



#### 参考：

1、[详解机器学习中的熵、条件熵、相对熵和交叉熵](https://zhuanlan.zhihu.com/p/35379531)

